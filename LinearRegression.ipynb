{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "In this, our equation is y(x) = w0+ w1 * x where y is the target variable, x is the input, and w0 and w1 is the parameters of the model. <br>\n",
    "As we can see in the graphs below, there are different lines for different values of w0 and w1: <br>\n",
    "![title](images/simpleLR.png)\n",
    "\n",
    "### Multiple Linear Regression\n",
    "In simple linear regression, we have one input variable. However, multiple linear regression has multiple input variables. In this case, our formula is: ![title](images/linearRegressionFormula.png)\n",
    "With this formula, our graph will be on n-dimensional space.\n",
    "\n",
    "### Cost Function\n",
    "Cost function, also known as squared error function, is used to measure the accuracy of our hypothesis.\n",
    "![title](images/costFunc.png)\n",
    "Cost function is all about distance between true target(points) and predicted target(regression line)\n",
    "![title](images/residual.png)\n",
    "We can minimize the cost function by adjusting the parameters.\n",
    "\n",
    "### Gradient Descent\n",
    "Gradient descent is used to minimize cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2264999999999997"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating squared error function (cost function)\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1.1, 2, 3.2])\n",
    "y = np.array([-1, 0.5, 1.7])\n",
    "\n",
    "def f(x,w):\n",
    "    return x*w\n",
    "\n",
    "def E(w, y, x):\n",
    "    return np.sum((y-f(x,w))**2)\n",
    "\n",
    "E(0.1, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 6.885341879776147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.97911714, 0.50278635])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#minimizing the cost function\n",
    "x = np.array([8.0 , 6.1 , 11.,  7.,   9.,   12. , 4.,   2.,   10,    5,    3])\n",
    "y = np.array([6.04, 4.95, 5.58, 6.81, 6.33, 7.96, 5.24, 2.26, 8.84, 2.82, 3.68])\n",
    "\n",
    "def error(y, x, w):\n",
    "    N = len(y)\n",
    "    f = x*w[1]+w[0]\n",
    "    e = y-f\n",
    "    return np.sum(e*e)/2\n",
    "\n",
    "w = np.array([0,0]) #for w=[0,0], E=187.06635\n",
    "E = error(y,x,w)\n",
    "\n",
    "#this is all about finding the minimum error.\n",
    "#with this 1000 iteration, we find E=6.886976324033383 for w=[1.98292448, 0.50406519]\n",
    "for e in range(1000):\n",
    "    g = 0.1*np.random.randn(2)   \n",
    "    w_temp = w + g\n",
    "    E_temp = error(y, x, w_temp)\n",
    "    if E_temp<E:\n",
    "        E = E_temp\n",
    "        w = w_temp\n",
    "        #print(e, E)\n",
    "print(e, E)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.32392 22.484934047999996 1.4888\n",
      "24.408828720000002 11.934959862173088 1.1355608\n",
      "16.86650064552 6.897547637950268 0.8914725127999998\n",
      "11.654751946054317 4.492279011716131 0.7228075063447998\n",
      "8.053433594723531 3.343808942793229 0.6062599868842566\n",
      "5.564922613953959 2.795436305813854 0.5257256509370213\n",
      "3.8453615262421854 2.5335987907363045 0.4700764247974817\n",
      "2.6571448146333503 2.408576352199561 0.4316228095350598\n",
      "1.8360870669116438 2.3488805132245987 0.4050513613887263\n",
      "1.2687361632359462 2.3203768843349946 0.38669049071960987\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1.1, 2, 3.2])\n",
    "y = np.array([-1, 0.5, 1.7])\n",
    "\n",
    "def f(x,w):\n",
    "    return x*w\n",
    "\n",
    "#gradient\n",
    "def dE(w,y,x):\n",
    "    return np.sum(-2*(y-f(x,w))*x)\n",
    "\n",
    "def E(w, y, x):\n",
    "    return np.sum((y-f(x,w))**2)\n",
    "\n",
    "#E(0.1, y, x)\n",
    "\n",
    "#0.2 ile negatif çıktı.\n",
    "#dE(0.2,y,x)\n",
    "\n",
    "eta =0.01 #learning algorithm\n",
    "w = 2\n",
    "for e in range(10):\n",
    "    w = w - eta*dE(w,y,x)\n",
    "    print(dE(w,y,x), E(w,y,x),w)\n",
    "\n",
    "#gradient of the error with respect w.\n",
    "# E(w) = (-1 - 1.1*w )**2 + (0.5 - 2*w)**2 + (1.7 - 3.2*w)**2\n",
    "#gradient with respect to w = derivative of E: 2*(-1 - 1.1*w)*(-1.1) + 2*(0.5 - 2*w)(-2) + 2*(1.7 - 3.2*w)(-3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
